var documenterSearchIndex = {"docs":
[{"location":"summarization/#Summarization-Metrics","page":"Summarization","title":"Summarization Metrics","text":"","category":"section"},{"location":"summarization/","page":"Summarization","title":"Summarization","text":"Metric Functions to evaulate models for the task for Summarization.","category":"page"},{"location":"summarization/#Rouge-Score","page":"Summarization","title":"Rouge Score","text":"","category":"section"},{"location":"summarization/","page":"Summarization","title":"Summarization","text":"NLPMetrics.rouge_n\nNLPMetrics.rouge_l_sentence_level\nNLPMetrics.rouge_l_summary_level\nNLPMetrics.rouge","category":"page"},{"location":"summarization/#NLPMetrics.rouge_n","page":"Summarization","title":"NLPMetrics.rouge_n","text":"rouge_n(evaluated_sentences, reference_sentences; n=2)\n\nComputes ROUGE-N of two text collections of sentences. Returns f1, precision, recall for ROUGE-N.\n\nArguments:\n\nevaluated_sentences: the sentences that have been picked by the summarizer\nreference_sentences: the sentences from the referene set\nn: size of ngram.  Defaults to 2.\n\nSource: (http://research.microsoft.com/en-us/um/people/cyl/download/   papers/rouge-working-note-v1.3.1.pdf)\n\n\n\n\n\n","category":"function"},{"location":"summarization/#NLPMetrics.rouge_l_sentence_level","page":"Summarization","title":"NLPMetrics.rouge_l_sentence_level","text":"rouge_l_sentence_level(evaluated_sentences, reference_sentences)\n\nComputes ROUGE-L (sentence level) of two text collections of sentences.\n\nCalculated according to:   Rlcs = LCS(X,Y)/m,   Plcs = LCS(X,Y)/n,   Flcs = ((1 + beta^2)*Rlcs*Plcs) / (Rlcs + (beta^2) * P_lcs)\n\nwhere:   X = reference summary   Y = Candidate summary   m = length of reference summary   n = length of candidate summary\n\nArgumnets:\n\nevaluated_sentences: the sentences that have been picked by the summarizer\nreference_sentences: the sentences from the referene set\n\nSource: (http://research.microsoft.com/en-us/um/people/cyl/download/papers/rouge-working-note-v1.3.1.pdf)\n\n\n\n\n\n","category":"function"},{"location":"summarization/#NLPMetrics.rouge_l_summary_level","page":"Summarization","title":"NLPMetrics.rouge_l_summary_level","text":"rouge_l_summary_level(evaluated_sentences, reference_sentences)\n\nComputes ROUGE-L (summary level) of two text collections of sentences.\n\nCalculated according to:   Rlcs = SUM(1, u)[LCS<union>(ri,C)]/m   Plcs = SUM(1, u)[LCS<union>(ri,C)]/n   Flcs = ((1 + beta^2)*Rlcs*Plcs) / (Rlcs + (beta^2) * P_lcs)\n\nwhere:   SUM(i,u) = SUM from i through u   u = number of sentences in reference summary   C = Candidate summary made up of v sentences   m = number of words in reference summary   n = number of words in candidate summary\n\nArguments:\n\nevaluated_sentences: the sentences that have been picked by the summarizer\nreference_sentence: the sentences in the reference summaries\n\nSource: (http://research.microsoft.com/en-us/um/people/cyl/download/papers/rouge-working-note-v1.3.1.pdf)\n\n\n\n\n\n","category":"function"},{"location":"summarization/#NLPMetrics.rouge","page":"Summarization","title":"NLPMetrics.rouge","text":"rouge(hypotheses, references)\n\nCalculates average rouge scores for a list of hypotheses and references.\n\n\n\n\n\n","category":"function"},{"location":"tut/#Tutorials","page":"Tutorials","title":"Tutorials","text":"","category":"section"},{"location":"tut/","page":"Tutorials","title":"Tutorials","text":"To be updated soon...","category":"page"},{"location":"classification/#Classification","page":"Classification","title":"Classification","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"Here are some of the metrics for classifications tasks in NLP.","category":"page"},{"location":"classification/#Confusion-Matrix","page":"Classification","title":"Confusion Matrix","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"NLPMetrics.confusion_matrix","category":"page"},{"location":"classification/#NLPMetrics.confusion_matrix","page":"Classification","title":"NLPMetrics.confusion_matrix","text":"confusion_matrix(y_pred, y_true)\n\nFunction to create a confusionmatrix for classification problems based on provided `ypredandytrue. Expectsytrueandypred`, to be onehotenocded already.\n\n\n\n\n\n","category":"function"},{"location":"classification/#Precision","page":"Classification","title":"Precision","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"NLPMetrics.precision","category":"page"},{"location":"classification/#NLPMetrics.precision","page":"Classification","title":"NLPMetrics.precision","text":"precision(y_pred, y_true; avg_type=\"macro\", sample_weights=nothing)\n\nComputes the precision of the predictions with respect to the labels. \n\nArguments\n\ny_pred: predicted values.\ny_true: ground truth values on the basis of which predicted values are to be assessed. Expects it to be one-hot encoded already\navg_type=\"macro\": Type of average to be used while calculating precision of multiclass models. Can take values as macro, micro and weighted. Default set to macro.\nsample_weights: Class weights to be provided when avg_type is set to weighted. Useful in case of imbalanced classes.\n\n\n\n\n\n","category":"function"},{"location":"classification/#Recall","page":"Classification","title":"Recall","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"NLPMetrics.recall","category":"page"},{"location":"classification/#NLPMetrics.recall","page":"Classification","title":"NLPMetrics.recall","text":"recall(y_pred, y_true; avg_type=\"macro\", sample_weights=nothing)\n\nComputes the recall of the predictions with respect to the labels.\n\nArguments\n\ny_pred: predicted values. \ny_true: ground truth values on the basis of which predicted values are to be assessed. Expects it to be one-hot encoded already.\navg_type=\"macro\": Type of average to be used while calculating precision of multiclass models. Can take values as macro, micro and weighted. Default set to macro.\nsample_weights: Class weights to be provided when avg_type is set to weighted. Useful in case of imbalanced classes.\n\nAliases: sensitivity and detection_rate\n\n\n\n\n\n","category":"function"},{"location":"classification/#F_beta-Score","page":"Classification","title":"F_beta Score","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"NLPMetrics.f_beta_score","category":"page"},{"location":"classification/#NLPMetrics.f_beta_score","page":"Classification","title":"NLPMetrics.f_beta_score","text":"f_beta_score(y_pred, y_true; β=1, avg_type=\"macro\", sample_weights=nothing)\n\nCompute f-beta score. The F_beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.\n\nArguments\n\ny_pred: predicted values.\ny_true: ground truth values on the basis of which predicted values are to be assessed. Expects it to be one-hot encoded already\nβ=1: the weight of precision in the combined score. If β<1, more weight given to precision, while β>1 favors recall.\navg_type=\"macro\": Type of average to be used while calculating precision of multiclass models. Can take values as macro, micro and weighted. Default set to macro.\nsample_weights: Class weights to be provided when avg_type is set to weighted. Useful in case of imbalanced classes.\n\n\n\n\n\n","category":"function"},{"location":"tm/#Translation-Metrics","page":"Translation","title":"Translation Metrics","text":"","category":"section"},{"location":"tm/","page":"Translation","title":"Translation","text":"Metric Functions to evaulate models for the task for Machine Translation.","category":"page"},{"location":"tm/#BLEU-Score","page":"Translation","title":"BLEU Score","text":"","category":"section"},{"location":"tm/","page":"Translation","title":"Translation","text":"NLPMetrics.bleu_score","category":"page"},{"location":"tm/#NLPMetrics.bleu_score","page":"Translation","title":"NLPMetrics.bleu_score","text":"bleu_score(reference_corpus, translation_corpus; max_order=4, smooth=false)\n\nComputes BLEU score of translated segments against one or more references. Returns the BLEU score, n-gram precisions, brevity penalty,  geometric mean of n-gram precisions, translationlength and  referencelength\n\nArguments\n\nreference_corpus: list of lists of references for each translation. Each reference should be tokenized into a list of tokens.\ntranslation_corpus: list of translations to score. Each translation should be tokenized into a list of tokens.\nmax_order: maximum n-gram order to use when computing BLEU score. \nsmooth=false: whether or not to apply. Lin et al. 2004 smoothing.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = NLPMetrics","category":"page"},{"location":"#NLPMetrics","page":"Home","title":"NLPMetrics","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"NLPMetrics is a Julia based package to facilitate evaluation of common NLP Tasks like Translation, Generation, Classification and Summarization.","category":"page"},{"location":"#Why-NLPMetrics?","page":"Home","title":"Why NLPMetrics?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To be updated soon...","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To be updated soon...","category":"page"},{"location":"#Example-Use-Case","page":"Home","title":"Example Use-Case","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To be updated soon...","category":"page"}]
}
