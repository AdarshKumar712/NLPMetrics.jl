var documenterSearchIndex = {"docs":
[{"location":"general/#General","page":"General","title":"General","text":"","category":"section"},{"location":"general/","page":"General","title":"General","text":"Here are some of the metrics for in-general use case like precision, recall, accuracy.","category":"page"},{"location":"general/","page":"General","title":"General","text":"(to be updated soon)","category":"page"},{"location":"general/#Accuracy","page":"General","title":"Accuracy","text":"","category":"section"},{"location":"general/#Precision","page":"General","title":"Precision","text":"","category":"section"},{"location":"general/#Recall","page":"General","title":"Recall","text":"","category":"section"},{"location":"general/#F1-Score","page":"General","title":"F1 Score","text":"","category":"section"},{"location":"general/#Confusion-Matrix","page":"General","title":"Confusion Matrix","text":"","category":"section"},{"location":"summarization/#Summarization-Metrics","page":"Summarization","title":"Summarization Metrics","text":"","category":"section"},{"location":"summarization/","page":"Summarization","title":"Summarization","text":"Metric Functions to evaulate models for the task for Summarization.","category":"page"},{"location":"summarization/#Rouge-Score","page":"Summarization","title":"Rouge Score","text":"","category":"section"},{"location":"summarization/","page":"Summarization","title":"Summarization","text":"NLPMetrics.rouge_n\nNLPMetrics.rouge_l_sentence_level\nNLPMetrics.rouge_l_summary_level\nNLPMetrics.rouge","category":"page"},{"location":"summarization/#NLPMetrics.rouge_n","page":"Summarization","title":"NLPMetrics.rouge_n","text":"rouge_n(evaluated_sentences, reference_sentences; n=2)\n\nComputes ROUGE-N of two text collections of sentences. Returns f1, precision, recall for ROUGE-N.\n\nArguments:\n\nevaluated_sentences: the sentences that have been picked by the summarizer\nreference_sentences: the sentences from the referene set\nn: size of ngram.  Defaults to 2.\n\nSource: (http://research.microsoft.com/en-us/um/people/cyl/download/   papers/rouge-working-note-v1.3.1.pdf)\n\n\n\n\n\n","category":"function"},{"location":"summarization/#NLPMetrics.rouge_l_sentence_level","page":"Summarization","title":"NLPMetrics.rouge_l_sentence_level","text":"rouge_l_sentence_level(evaluated_sentences, reference_sentences)\n\nComputes ROUGE-L (sentence level) of two text collections of sentences.\n\nCalculated according to:   Rlcs = LCS(X,Y)/m,   Plcs = LCS(X,Y)/n,   Flcs = ((1 + beta^2)*Rlcs*Plcs) / (Rlcs + (beta^2) * P_lcs)\n\nwhere:   X = reference summary   Y = Candidate summary   m = length of reference summary   n = length of candidate summary\n\nArgumnets:\n\nevaluated_sentences: the sentences that have been picked by the summarizer\nreference_sentences: the sentences from the referene set\n\nSource: (http://research.microsoft.com/en-us/um/people/cyl/download/papers/rouge-working-note-v1.3.1.pdf)\n\n\n\n\n\n","category":"function"},{"location":"summarization/#NLPMetrics.rouge_l_summary_level","page":"Summarization","title":"NLPMetrics.rouge_l_summary_level","text":"rouge_l_summary_level(evaluated_sentences, reference_sentences)\n\nComputes ROUGE-L (summary level) of two text collections of sentences.\n\nCalculated according to:   Rlcs = SUM(1, u)[LCS<union>(ri,C)]/m   Plcs = SUM(1, u)[LCS<union>(ri,C)]/n   Flcs = ((1 + beta^2)*Rlcs*Plcs) / (Rlcs + (beta^2) * P_lcs)\n\nwhere:   SUM(i,u) = SUM from i through u   u = number of sentences in reference summary   C = Candidate summary made up of v sentences   m = number of words in reference summary   n = number of words in candidate summary\n\nArguments:\n\nevaluated_sentences: the sentences that have been picked by the summarizer\nreference_sentence: the sentences in the reference summaries\n\nSource: (http://research.microsoft.com/en-us/um/people/cyl/download/papers/rouge-working-note-v1.3.1.pdf)\n\n\n\n\n\n","category":"function"},{"location":"summarization/#NLPMetrics.rouge","page":"Summarization","title":"NLPMetrics.rouge","text":"rouge(hypotheses, references)\n\nCalculates average rouge scores for a list of hypotheses and references.\n\n\n\n\n\n","category":"function"},{"location":"tm/#Translation-Metrics","page":"Translation","title":"Translation Metrics","text":"","category":"section"},{"location":"tm/","page":"Translation","title":"Translation","text":"Metric Functions to evaulate models for the task for Machine Translation.","category":"page"},{"location":"tm/#BLEU-Score","page":"Translation","title":"BLEU Score","text":"","category":"section"},{"location":"tm/","page":"Translation","title":"Translation","text":"NLPMetrics.bleu_score","category":"page"},{"location":"tm/#NLPMetrics.bleu_score","page":"Translation","title":"NLPMetrics.bleu_score","text":"bleu_score(reference_corpus, translation_corpus; max_order=4, smooth=false)\n\nComputes BLEU score of translated segments against one or more references. Returns the BLEU score, n-gram precisions, brevity penalty,  geometric mean of n-gram precisions, translationlength and  referencelength\n\nArguments\n\nreference_corpus: list of lists of references for each translation. Each reference should be tokenized into a list of tokens.\ntranslation_corpus: list of translations to score. Each translation should be tokenized into a list of tokens.\nmax_order: maximum n-gram order to use when computing BLEU score. \nsmooth=false: whether or not to apply. Lin et al. 2004 smoothing.\n\n\n\n\n\n","category":"function"},{"location":"tut/#Tutorials","page":"Tutorials","title":"Tutorials","text":"","category":"section"},{"location":"tut/","page":"Tutorials","title":"Tutorials","text":"To be updated soon...","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = NLPMetrics","category":"page"},{"location":"#NLPMetrics","page":"Home","title":"NLPMetrics","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"NLPMetrics is a Julia based package to facilitate evaluation of common NLP Tasks like Translation, Generation, Classification and Summarization.","category":"page"},{"location":"#Why-NLPMetrics?","page":"Home","title":"Why NLPMetrics?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To be updated soon...","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To be updated soon...","category":"page"},{"location":"#Example-Use-Case","page":"Home","title":"Example Use-Case","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To be updated soon...","category":"page"}]
}
